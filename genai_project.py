# -*- coding: utf-8 -*-
"""Genai-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZbjGTcUOQV2R9WgmarfzaZ2HE2epFhmW
"""



"""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"""

!pip install langchain-core langchain_groq langchain_community

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

response = llm.invoke("What is Ganesha?")
print(response.content)

from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://www.google.com/about/careers/applications/jobs/results/141544156919931590-software-engineer-phd-early-career-campus-2025-start")
page_data = loader.load().pop().page_content
print(page_data)

from langchain_core.prompts import PromptTemplate
prompt_extract = PromptTemplate.from_template("""
        ### SCRAPED TEXT FROM WEBSITE:
        {page_data}
        ### INSTRUCTION:
        The scraped text is from the career's page of a website.
        Your job is to extract the job postings and return them in JSON format containing the
        following keys: `role`, `experience`, `skills` and `description`.
        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):
        """)
chain_extract = prompt_extract | llm
res = chain_extract.invoke(input = {'page_data': page_data})
print(res.content)

from langchain_core.output_parsers import JsonOutputParser
json_parser = JsonOutputParser()
json_res = json_parser.parse(res.content)
print(json_res)

import pandas as pd
df = pd.read_csv("/content/portfolio.csv")
df

!pip install chromadb

import chromadb

client = chromadb.Client()
collections = client.create_collection(name="interview_2")

collections.add(
    documents = [
        "This document is about Kedarnath",
        "This document is about Maharastra"
    ],

    ids = ['id1', 'id2'],
    metadatas = [
        {"url": "https://shrikedarnathcharitabletrust.uk.gov.in/index.html"},
        {"url": "https://www.dagdushethganpati.com/"},
    ]
)

all_docs = collections.get()

all_docs

documents = collections.get(ids = ['id1'])
documents

results = collections.query(
    query_texts = ['Query is about varanasi'],
    n_results = 2
)

results

results = collections.query(
    query_texts = ['Query is about Ganpati Visarjan'],
    n_results = 2
)

results

import uuid
import chromadb

client = chromadb.PersistentClient()
collections = client.get_or_create_collection(name = "portfolio_data")

df

if not collections.count():
  for i, row in  df.iterrows():
    collections.add(documents = row['Technology'], ids =[str(uuid.uuid4())])

tech = collections.query(query_texts=['Experience in Python', 'React'], n_results = 2, ).get('documents', [])

tech

json_res

if type(json_res) == dict:
  job = json_res.get("skills", [])
else:
  job = json_res[0].get("skills", [])

job

prompt_skills_and_question = PromptTemplate.from_template("""
        ### JOB DESCRIPTION:
        {job_description}

        ### INSTRUCTION:
        You are Mishu Dhar Chando, the CEO of Knowledge Doctor, a YouTube channel specializing in educating individuals on machine learning, deep learning, and natural language processing.
        Your expertise lies in bridging the gap between theoretical knowledge and practical applications through engaging content and innovative problem-solving techniques.
        Your job is to:
        1. Analyze the given job description to identify the required technical skills and match them with the provided skill set to calculate a percentage match.
        2. Generate a list of relevant interview questions based on the job description.
        3. Return the information in JSON format with the following keys:
            - `skills_match`: A dictionary where each key is a skill, and the value is the matching percentage.
            - `interview_questions`: A list of tailored questions related to the job description.

        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):

        """)

chain_skills_and_question = prompt_skills_and_question | llm
res = chain_skills_and_question.invoke({"job_description": str(job)})
print(res.content)

!pip install gradio

import gradio as gr
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
import pandas as pd
import uuid
import chromadb
from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)

def preproces_job_posting(url, portfolio_csv):
  loader = WebBaseLoader(url)
  page_data = loader.load().pop().page_content
  prompt_extract = PromptTemplate.from_template("""
        ### SCRAPED TEXT FROM WEBSITE:
        {page_data}
        ### INSTRUCTION:
        The scraped text is from the career's page of a website.
        Your job is to extract the job postings and return them in JSON format containing the
        following keys: `role`, `experience`, `skills` and `description`.
        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):
        """)
  chain_extract = prompt_extract | llm
  res_1 = chain_extract.invoke(input = {'page_data': page_data})
  json_parser = JsonOutputParser()
  json_res = json_parser.parse(res_1.content)

  df = pd.read_csv(portfolio_csv)

  client = chromadb.PersistentClient('vectorstore')
  collections = client.get_or_create_collection(name="portfolio_app")
  if not collections.count():
    for i, row in  df.iterrows():
      collections.add(documents = row['Technology'], ids =[str(uuid.uuid4())])
  job  = json_res.get('skills', []) if type(json_res) == dict else json_res[0].get('skills', [])

  prompt_skills_and_question = PromptTemplate.from_template("""
        ### JOB DESCRIPTION:
        {job_description}

        ### INSTRUCTION:
        You are Ananya, the CEO of company, specializing in educating individuals on machine learning, deep learning, and natural language processing.
        Your expertise lies in bridging the gap between theoretical knowledge and practical applications through engaging content and innovative problem-solving techniques.
        Your job is to:
        1. Analyze the given job description to identify the required technical skills and match them with the provided skill set to calculate a percentage match.
        2. Generate a list of relevant interview questions based on the job description.
        3. Return the information in JSON format with the following keys:
            - `skills_match`: A dictionary where each key is a skill, and the value is the matching percentage.
            - `interview_questions`: A list of tailored questions related to the job description.

        Only return the valid JSON.
        ### VALID JSON (NO PREAMBLE):

        """)
  chain_skills_and_question = prompt_skills_and_question | llm
  res1 = chain_skills_and_question.invoke({"job_description": str(job)})
  final_result = json_parser.parse(res1.content)
  return final_result

def gradio_interface(url, portfolio_csv):
  try:
    result = preproces_job_posting(url, portfolio_csv)
    return result
  except Exception as e:
    return {"error": str(e)}

with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
  gr.Markdown("# Job Scraping & Analyzer with Interview Preparation Questions Using Gen-AI")

  with gr.Row():
    url_input = gr.Textbox(label = "Website URL", placeholder="Enter the url of the job posting")
    portfolio_input = gr.File(label = "Upload Portfolio CSV")

  analyze_button = gr.Button("Analyze Job Posting")
  output_box = gr.JSON(label = "Result")

  analyze_button.click(gradio_interface, inputs = [url_input, portfolio_input], outputs = output_box)

app.launch()



